{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCxvTqFdeCLzxhBvHWOmHa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AleDan-Facile/Script_entita_da_condividere/blob/main/Da_Condividere_Script_Analisi_Entit%C3%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script per l'analisi delle entità nei testi - Facile.it\n",
        "\n",
        "**IMPORTANTE: FAI UNA COPIA DEL FILE PER POTERLO USARE**\n",
        "\n",
        "## Occorrente:\n",
        "- File di autorizzazione per usare le API di Google, caricato in una cartella Drive di tua scelta.\n",
        "- Un file CSV con due colonne: \"URL\" e \"Content\". \"URL\" serve per capire a quale URL, appunto, appartiene il contenuto. \"Content\" per il contenuto da analizzare. Puoi ottenere questo file a partire da una scansione di Screaming Frog con estrazione personalizzata, o con il metodo di scraping che preferisci. **Ricorda di caricare subito il file**.\n",
        "\n",
        "### Impostazioni:\n",
        "\n",
        "Nel primo blocco di codice troverai tre campi da impostare:\n",
        "\n",
        "- lang: È il parametro che si riferisce alla lingua. Usa un input di codice linguistico a due caratteri. L'italiano è \"it\", ed è preimpostato.\n",
        "- file: È il percorso temporaneo del CSV da analizzare. Ti basterà impostare: {{nome_file}} + \".csv\"\n",
        "- auth_path: È il percorso del file di autorizzazione JSON delle API di Google Cloud, che dovrai aver preventivamente salvato in Google Drive. Copia il percorso del file ed inseriscilo dove richiesto.\n",
        "\n",
        "### Come far funzionare lo script:\n",
        "\n",
        "Dopo aver impostato quanto richiesto, ti basta far partire il codice così come indicato per ottenere i CSV di risultato.\n",
        "Otterrai diversi CSV, che dovrai combinare in un solo file per arrivare al risultato finale.\n",
        "\n",
        "### Next Step:\n",
        "\n",
        "Ci piacerebbe sapere cosa pensi che si possa migliorare di questo file, in modo da rendere il lavoro sempre più snello e veloce. Al momento, abbiamo in cantiere:\n",
        "- Implementazione dello scraping via Trafilatura per essere indipendenti dal CSV, e passare direttamente un DataFrame alle API. Questo per snellire il processo e non uscire nemmeno dal Colab, anche in fase di preparazione dei dati."
      ],
      "metadata": {
        "id": "J-pI35EAEdkS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3NXv95eC7RU"
      },
      "outputs": [],
      "source": [
        "#@title Esegui questo blocco di codice per primo. Accetta la richiesta di accesso a Google Drive.\n",
        "\n",
        "# Questo codice aggiorna alcune librerie che vengono richiamate nello script, e monta drive nell'attuale runtime.\n",
        "# Montare drive serve a recuperare la cartella dove devi aver preventivamente inserito la chiave di autorizzazione JSON per utilizzare le API.\n",
        "# Puoi aprire una chiave di autorizzazione qui: https://console.cloud.google.com/\n",
        "\n",
        "!pip install --upgrade google-cloud-language\n",
        "!pip install --upgrade requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imposta i parametri richiesti (Descrizione dei campi come commento)\n",
        "\n",
        "# lang è la lingua: devi inserire solo una sigla formata da due lettere. L'italiano \"it\" è inserito di default.\n",
        "lang = 'it' #@param {type:\"string\"}\n",
        "\n",
        "# file è il percorso del file CSV che devi caricare per l'analisi\n",
        "file = 'input.csv' #@param {type:\"string\"}\n",
        "\n",
        "# auth_path è il percorso del file di autorizzazione JSON del tuo Google Drive. Incollalo qui.\n",
        "auth_path = '/xyz/xyz/xyz.json' #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R0AD1rRLEocK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Esegui dopo aver impostato i parametri. Gli output saranno tra i files con nome \"output.csv\", \"output_mentions.csv\"; \"output_salience.csv\" e un file con la lista delle URL analizzate.\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=auth_path\n",
        "\n",
        "#ensure the path is set correctly\n",
        "!echo $GOOGLE_APPLICATION_CREDENTIALS\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "\n",
        "from google.cloud import language\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import language_v1\n",
        "\n",
        "def analyze_entities(text_content):\n",
        "    \"\"\"\n",
        "    Analyzing Entities in a String\n",
        "\n",
        "    Args:\n",
        "      text_content The text content to analyze\n",
        "    \"\"\"\n",
        "\n",
        "    # Available types: PLAIN_TEXT, HTML\n",
        "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
        "\n",
        "    # Optional. If not specified, the language is automatically detected.\n",
        "    # For list of supported languages:\n",
        "    # https://cloud.google.com/natural-language/docs/languages\n",
        "    language = lang\n",
        "    document = {\"content\": text_content, \"type_\": type_, \"language\": language}\n",
        "\n",
        "    # Available values: NONE, UTF8, UTF16, UTF32\n",
        "    encoding_type = language_v1.EncodingType.UTF8\n",
        "\n",
        "    response = client.analyze_entities(request = {'document': document, 'encoding_type': encoding_type})\n",
        "\n",
        "    # Loop through entitites returned from the API\n",
        "\n",
        "    aOutput = []\n",
        "    aEntitiesTypeExclusion = ['PRICE','DATE','ADDRESS','NUMBER','PHONE_NUMBER']\n",
        "    for entity in response.entities:\n",
        "        \n",
        "        if(language_v1.Entity.Type(entity.type_).name in aEntitiesTypeExclusion):\n",
        "          continue\n",
        "\n",
        "        salience = entity.salience\n",
        "        metas = []\n",
        "        mentions = []\n",
        "        for metadata_name, metadata_value in entity.metadata.items():\n",
        "            metas.append([metadata_name, metadata_value])\n",
        "\n",
        "        # Loop over the mentions of this entity in the input document.\n",
        "        # The API currently supports proper noun mentions.\n",
        "        for mention in entity.mentions:\n",
        "            #print(mention)\n",
        "            mentions.append(mention.text.content+\" - \"+str(mention.text.begin_offset))\n",
        "\n",
        "        #aOutput[0].append([entity.name,language_v1.Entity.Type(entity.type_).name])\n",
        "        aOutput.append([entity.name,language_v1.Entity.Type(entity.type_).name,metas,mentions,entity.salience])\n",
        "\n",
        "    return aOutput\n",
        "\n",
        "def analyze_syntax(text_content):\n",
        "  type_ = language_v1.Document.Type.PLAIN_TEXT\n",
        "  language = lang\n",
        "  document = {\"content\": text_content, \"type_\": type_, \"language\": language}\n",
        "\n",
        "  # Available values: NONE, UTF8, UTF16, UTF32\n",
        "  encoding_type = language_v1.EncodingType.UTF8\n",
        "  aOutputSyntax = []\n",
        "  response = client.analyze_syntax(request = {'document': document, 'encoding_type': encoding_type})\n",
        "  for sentence in response.sentences:\n",
        "    text = sentence.text\n",
        "    aOutputSyntax.append([sentence.text.content,str(sentence.text.begin_offset)])\n",
        "  return aOutputSyntax\n",
        "\n",
        "\n",
        "client = language_v1.LanguageServiceClient()\n",
        "\n",
        "df = pd.read_csv(file)\n",
        "aOutput = dict()\n",
        "aOutputSailence = dict()\n",
        "aOutputMentions = dict()\n",
        "aOutputSentences = dict()\n",
        "entities = []\n",
        "for label, row in df.iterrows():\n",
        "    print(\"analyzing===>\")\n",
        "    print(row[0])\n",
        "    a = []\n",
        "    aSailence = dict()\n",
        "    aMentions = dict()\n",
        "    #aEntities = analyze_entities(row[2].lower())\n",
        "    #aSentences = analyze_syntax(row[2].lower())\n",
        "    #Qui viene analizzato il testo\n",
        "    aEntities = analyze_entities(row[1])\n",
        "    aSentences = analyze_syntax(row[1])\n",
        "    for aEntity in aEntities:\n",
        "      entityKey = aEntity[0]+\" - \"+aEntity[1]\n",
        "      if(entityKey in a):\n",
        "        continue;\n",
        "      a.append(entityKey)\n",
        "      aSailence[entityKey] = aEntity[4]\n",
        "      aMentions[entityKey] = \";\".join(aEntity[3])\n",
        "    #print(aSailence)\n",
        "    #Qui ricostruisce output\n",
        "    aOutput[row[0]] = a\n",
        "    aOutputSailence[row[0]] = aSailence\n",
        "    aOutputMentions[row[0]] = aMentions\n",
        "    aOutputSentences[row[0]] = aSentences\n",
        "#concludere a stampa su csv dei doc di appoggio sailence e mentions\n",
        "    \n",
        "    #print(aOutputMentions)\n",
        "    in_first = set(entities)\n",
        "    in_second = set(a)\n",
        "    in_second_but_not_in_first = in_second - in_first\n",
        "    entities = entities + list(in_second_but_not_in_first)\n",
        "    time.sleep(0.10)\n",
        "#print(aOutputSailence)\n",
        "result = {}\n",
        "aTest = dict()\n",
        "aSailence = dict()\n",
        "aMentions = dict()\n",
        "\n",
        "for key in aOutput.keys():\n",
        "  aAux = dict()\n",
        "  aAuxSailence = dict()\n",
        "  aAuxMentions = dict()\n",
        "  for val in entities:\n",
        "    bInValues = 0\n",
        "    if val in aOutput[key]:\n",
        "      bInValues = 1\n",
        "    aAux[val] = bInValues\n",
        "    aAuxSailence[val] = aOutputSailence[key].get(val,0)\n",
        "#    if val in aOutputSailence[key]:\n",
        "#      aAuxSailence[val] = aOutputSailence[key][val]\n",
        "    aAuxMentions[val] = aOutputMentions[key].get(val,\"\")\n",
        "#    if val in aOutputMentions[key]:\n",
        "#      aAuxMentions[val] = aOutputMentions[key][val]\n",
        "  aTest[key] = aAux\n",
        "  aSailence[key] = aAuxSailence\n",
        "  aMentions[key] = aAuxMentions\n",
        "dfAux = pd.DataFrame(aOutputSentences[key])\n",
        "sSlug = re.sub('[^0-9a-zA-Z]+', '*', key)\n",
        "sFileName = '/content/'+sSlug+'_sentences.csv'\n",
        "dfAux.to_csv(sFileName)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(aTest)\n",
        "\n",
        "df.to_csv('/content/output.csv')\n",
        "\n",
        "df2 = pd.DataFrame(aSailence)\n",
        "df2.to_csv('/content/output_sailence.csv')\n",
        "\n",
        "df3 = pd.DataFrame(aMentions)\n",
        "df3.to_csv('/content/output_mentions.csv')"
      ],
      "metadata": {
        "id": "H-TYBxR_DrtP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}